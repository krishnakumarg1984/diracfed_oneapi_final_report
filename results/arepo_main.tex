% chktex-file 13

\documentclass[../main]{subfiles}

\begin{document}

\subsection{AREPO}\label{sec:arepo}
Arepo is a massively parallel astrophysics code for the simulation of gravitational N-body systems and magnetohydrodynamics, both on Newtonian as well as cosmological backgrounds.
There are a number of versions in the community, originating from a common closed-source code base.
A version with reduced functionality was made publicly available under GPLv3\cite{weinberger_arepo_2020, springel_arepo_nodate}.
Different research groups use code bases derived from the closed source version, including the Sijaki group at the University of Cambridge.
This code~\cite{sijaki_arepo_nodate}, which was part of the DiRAC3 procurement, acceptance testing and technical commissioning, forms the basis of porting efforts to be undertaken within the course of this project.

In Arepo, the computational domain is discretized using a fully adaptive, dynamic unstructured Voronoi mesh, which is moving with the fluid in a quasi-Lagrangian way and that is paired with a finite volume approach for the hydrodynamics.
Arepo is written in C, parallelized with MPI and some of the code bases, including the one used in this project, have additionally OpenMP shared memory parallelization.
To the best of our knowledge, there is currently no version of this code that can target accelerators with any of the computational kernels in the main time loop.

\subsubsection{Running the code}
Arepo depends on several libraries that are necessary for compilation.
\begin{itemize}
	\item{\textbf{GSL\footnote{\url{https://www.gnu.org/software/gsl}}:}} GNU Scientific Library as a random number generator and for numerical integration.
	\item{\textbf{GMP\footnote{\url{https://gmplib.org/}}:}} GNU Multiple Precision Arithmetic Library to calculate exact geometric predictions
	\item{\textbf{FFTW3\footnote{\url{http://www.fftw.org/}}:}} Fastest Fourier Transform in the West is used in the particle-mesh algorithm
	\item{\textbf{HDF5\footnote{\url{https://support.hdfgroup.org/HDF5/doc/H5.intro.html}}:}} Hierarchical Data Format is used as the standard input/output format in Arepo.
\end{itemize}
Arepo also needs a configuration and a parameter file for each example.
The parameter file contains the run-time options whereas the configuration file specifies compile time settings.
Special variables in the parameter file need to be modified for each platform, e.g.\ the maximum memory size, which should be around 95\% of the available RAM for every process. Also in the parameter file are problem specific variables and those that points to the correct data folder for initial condition files and output files.
Most Arepo branches come with a test suite of small examples to verify the code.
Furthermore Arepo was used in the Illustris project\footnote{\url{https://www.illustris-project.org/}} and publicly available test data~\cite{Nelson_2015} can be used to check the performance for longer test runs.

\subsubsection{Profiling with Vtune and Intel Trace Analyzer}
In contrast to codes described above, where suitable kernels for GPU execution have already been identified in prior porting efforts, this selection process has to be done as an additional first step for this purely CPU-based code.
Therefore, extensive profiling of the code base has been undertaken using one of the test cases that are also used to verify code correctness to analyse the potential of porting parts of Arepo to GPUs.
Hence, a first analysis was carried out using Intel VTune profiler and Intel Trace Analyzer, producing two key insights:

The first insight was that a significant share of the overall run time is spent waiting in a synchronisation step that uses global MPI communication.
The purpose of this step is to detect the need for program interruption to trigger saving the current state in a snapshot file and termination of the program.
Since there is no data dependence on this communication step, it is possible to reverse the logic of this routine, replace the respective MPI calls with their non-blocking counterparts and trigger the snapshot writing after the next time step.
This enables perfect overlap of computation and communication at the expense of a one time step lag in the snapshot writing.
Figure~\ref{fig:arepo_mpicom} shows the comparison of the blocking (top) vs the nonblocking (bottom) synchronisation step for a random selection of MPI tasks.
It is clearly visible that the MPI busy wait portion of the code (yellow), i.e.\ the time that a rank is waiting for others to reach the synchronisation point, is reduced in the graphical output where nonblocking communication is used.

\begin{figure}[htp]
	\centering
	\subfloat{ \includegraphics[clip,width=\textwidth]{arepo_blocking.png}}\\
	\subfloat{ \includegraphics[clip,width=\textwidth]{arepo_nonblocking.png}}
	\caption{Significant reduction of MPI Busy Wait Time (yellow) in the lower VTune Profiler graphic compared to the top version where blocking communication was used. Shown for a random selection of ranks}
	\label{fig:arepo_mpicom} % chktex 24
\end{figure}
To further test these findings, the Illustris 3 test case~\cite{Nelson_2015} was used.
Figure~\ref{fig:arepo_blockvsnonblock} shows the impact of this improvement across varying number of MPI tasks by comparing overall runtime of the original code variant (blue line) with the improved version (orange line).
As expected, the synchronisation overhead is negligible for smaller numbers of tasks where computational work dominates but becomes noticeable with more than~2000 tasks and incurs significant slow-down with~3000 tasks and more.
\begin{figure}[htp]
	\centering
	\includegraphics[clip,width=0.6\textwidth]{images/Arepo_blockingvsNonblocking.png}
	\caption{A comparison between the blocking and nonblocking synchronization step for }
	\label{fig:arepo_blockvsnonblock} % chktex 24
\end{figure}

Second profiling outcome and original motivation for this analysis was to understand the distribution of run time across the different parts of a time step and subsequently to identify suitable candidates for GPU porting.
The two most time-consuming parts proved to be the evaluation of gravitational forces with a Tree particle Mesh algorithm, which is carried out in two half steps at the beginning and end of a time step, and the creation of the Voronoi mesh in-between.

\subsubsection{oneAPI Offload Advisor}
The next step in the analysis was to use Intel's Offload Advisor to see if the previously identified time consuming parts contain any kernels that would readily benefit from GPU offloading.
However, with the current structure of the underlying algorithm the Offload Advisor judged every routine to suffer from too much overhead when ported to GPU.
Only relatively short loops within sub steps of the mesh creation routines were classified as offload candidates that could potentially see a minimal speed-up.

Overall, this suggests that for a successful GPU port of Arepo it will not be sufficient to port few individual kernels to achieve performance gains but requires a better understanding of the numerical method and possibly improve contouring in the algorithm to better capture compute intensive parts in sections with limited code footprint.


\subsubsection{Intel oneAPI Math Kernel Library}
Intel oneAPI Math Kernel Library offers an accelerator enabled version of Fast Fourier Transforms, as well as FFTW3 (and FFTW2) interface wrapper libraries to these functionalities.
As far as we are aware currently none of the third party libraries, that Arepo depends on, has accelerator enabled versions.
With the interface wrapper libraries oneMKL can be used as a drop in replacement for FFTW, hence no program source code change is necessary.
Using oneMKL does not degrade the CPU performance on CSD3's Icelake partition.
We will test the potential performance benefits on Intel GPUs further.

\subsubsection{Programming Model}
AREPO is written in C, parallelized with MPI and OpenMP.
The original intention was to expand the existing OpenMP parallelization to include offload capabilities, in line with Intel's recommendation for C programs.
While investigating the performance of the OpenMP shared memory parallelization of AREPO, functional problems affecting the correctness of results were identified.
With confirmation from the AREPO developers at the University of Cambridge we decided that at this stage it was not worth the effort to debug and fix the existing shared memory parallelization, hence opening the door to consider SYCL as an alternative to OpenMP 5 offload.


\subsubsection{Summary}
A prerequisite for any successful porting effort is a suitable control flow structure that exposes sufficient parallelism to map to the computational capabilities of GPUs.
For many codes that have been equipped with accelerator capabilities in the past, independent from the programming model, this code structure is already in place.
The extensive analysis work performed on AREPO suggests that this is not yet the case here and significant effort has to be invested into transforming the code first before meaningful gains can be expected from porting to GPUs.

Taking into account the profiling results, that the tree based force evaluation and the mesh generation are the most time consuming parts, as well as the findings from the Offload Advisor, that the current code structure is not suitable for easy code porting, we decided to look into restructuring the code to limit the dependencies of the expensive sections. This is now ongoing work and won't finish in the timeframe of this project. 



%As a C application that includes already OpenMP parallelization, we intend to follow Intel's recommendation to use OpenMP 5 offloading to target GPUs.
%For that, we are in active discussions with the developers at the University of Cambridge and take into account prior experience and publications in the scientific domain.


%\begin{itemize}
%    \item reference to paper to port
%    \item https://arxiv.org/abs/1610.07279
%    \item https://arxiv.org/abs/0907.3390
%    \item https://arxiv.org/abs/1909.07439
%\end{itemize}

\end{document}
