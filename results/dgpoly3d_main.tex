%!TEX root = ../main.tex

\subsection{dGpoly3D}\label{sec:dgpoly3d}

Discontinuous Galerkin (dG) methods on meshes consisting of polytopic elements have received considerable attention in recent years.
By combining advantages from both finite element methods (FEMs) and finite volume methods (FVMs) they allow the simple treatment of complicated computational geometries, ease of adaptivity and stability for non-self-adjoint PDE problems.
It has also been shown that they are applicable on extremely general meshes, consisting of general polytopic elements with an arbitrary number of faces and different local elemental polynomial degrees.
A basic feature of these methods is the use of physical frame polynomial bases and this, together with the highly involved quadrature requirements over polytopic elements, pose new algorithmic challenges in the context of matrix assembly.
The implementation of arbitrary order quadrature rules over polytopic domains is non-trivial with the most general and widely used approach being the subdivision of polytopic elements into basic simplicial sub-elements; standard quadrature rules are then employed on each sub-element.
\texttt{dgpoly3d} \cite{dong_gpu-accelerated_2021} is a CUDA implementation of the symmetric interior penalty dG method, for the linear system assembly step of second order advection-diffusion-reaction equations.

The code first subdivides each 3D polytopic element into simplicial sub-elements (tetrahedrons) and any co-hyperplanar 2D faces into simplicial sub-faces (triangles), it then precomputes the sparsity pattern of the stiffness and mass matrix and stores it in CSR format and finally populates the matrix with the quadrature values for each simplex of the simplicial subdivision.
Different kernels are called for each term of the bilinear form, since the workload of each term can be substantially different.
For example, the kernel that calculates the integral over the elements needs a 3D quadrature rule, compared to the kernel over the interior faces which needs a 2D quadrature rule and thus less integration points.
For this project we will port one of the kernels, the kernel over the elemental integrals.
This kernel spawns as many threads as there are tetrahedrons on the mesh and for each tetrahedron it computes the contribution of this simplex and writes it into the main block diagonal of the matrix using atomic operations.
Atomic operations are needed since threads with contiguous thread id's might belong in the same polyhedron and thus will try to update the same memory location simultaneously.

\subsubsection{Preparing the code}\label{sec:dgpoly3d_prep}
The code is a combination of Python scripts for the non computationally demanding parts along with native CUDA kernels for the matrix assembly process, called from within Python with the PyCUDA module.
The code was executed for different mesh sizes and with $p=2$ (the degree of the polynomial on every polyhedron) and input and output data to and from the kernel were captured and stored, both to be able to execute the kernel without having to do the initialization steps every time but also to be used in a unit test.

Since Intel's DPC++ Compatibility tool would not be able to port the CUDA API calls (written in PyCUDA), a minimal C++ code to drive the kernel was written.
This code reads the input and output data from disk, performs the memory transfers from and to the device, calls the kernel and finally the unit test.
The unit test checks if the matrix entries match within a tolerance with the original values and for this a C++ version of Python's float comparison \texttt{math.isclose()} function was written, that takes as parameters both a relative and an absolute tolerance and returns True if

\begin{verbatim}
abs(x - y) <= max(rel_tol * max(abs(x), abs(y)), abs_tol))
\end{verbatim}
otherwise it returns False.
For the following comparisons we've set the relative tolerance to $1\%$ and the absolute to $10^{-5}$.

\subsubsection{Porting the code}\label{sec:dgpoly3d_porting}
We ported the code using Intel's DPC++ Compatibility tool (\texttt{dpct}).
Porting completed without errors and the compatibility tool added inline comments to explain some of its decisions or even propose changes (for example in one place where we access with atomic operations an array residing in global memory, it proposed a change in case the array is in local memory instead).
We also automatically created a \texttt{Makefile} with the \texttt{--gen-build-script} flag.
However, compiling failed with the following error message:
\begin{verbatim}
  no known conversion from 'dpct::accessor<float,
  dpct::constant, 2>' 'float (*)[3]' for 1st argument
\end{verbatim}
The problem was found to be with the function call that evaluates the Legendre polynomials at a given point.
In the CUDA code, the coefficients of the Legendre polynomials are stored in the GPU's constant memory
\begin{verbatim}
  __constant__ float legendre[3][3];
\end{verbatim}
and are passed to the \texttt{eval1dLegendre0()} and \texttt{eval1dLegendre1()} function calls to evaluate the Legendre polynomial and its first derivative and the function declaration is
\begin{verbatim}
  __device__ float eval1dLegendre0(float legendre[][3], int n, float x);
\end{verbatim}
After porting the code, while the declaration of the constant memory was changed to
\begin{verbatim}
  dpct::constant_memory<float, 2> legendre(3, 3);
\end{verbatim}
and it was included in the kernel call as
\begin{verbatim}
  dpct::accessor<float, dpct::constant, 2> legendre
\end{verbatim}
the type was not changed in the \texttt{eval1dLegendre} function.
After changing the parameter type for the first argument in the function definition from \texttt{float legendre[][3]} to \texttt{dpct::accessor<float, dpct::constant, 2> legendre} the ported code compiled succesfully.

\subsubsection{Running the code}\label{sec:dgpoly3d_running}
To start with, the simplified native CUDA code was tested on a workstation with an Nvidia 2060 (Turing) and in an HPC GPU cluster with A100's (Ampere).
In the first case we used CUDA SDK 11.6 and on the GPU cluster we compiled with CUDA SDK 11.4.
We also compiled it with the optimization levels \texttt{-O0} up to \texttt{-O3} and with or without the \texttt{-use\_fast\_math} flag and in all cases the unit test passed with no errors.

Now, for the ported SYCL/DPC++ code multiple different platforms were used, both CPU based and GPU based.
The GPU was once again the Nvidia Ampere A100 and the CPU based systems were one with dual Intel Icelake 8368Q and one with dual AMD EPYC 7763.
To target the Nvidia GPU we compiled with the LLVM CUDA backend compiler with \texttt{clang++ -fsycl -fsycl-targets=nvptx64-nvidia-cuda} and to target the CPU we compiled with both the DPC++ compiler (\texttt{dpcpp}) and with Intel's LLVM with \texttt{clang++ -fsycl} (which by default selects the generic spir64 target).
The results of the unit test can be found on Table \ref{tab:correctness}.

\begin{center}
	\begin{table}
		\begin{tabular}{||c c c c c||}
			\hline
			    & \multicolumn{2}{c}{dpcpp} & \multicolumn{2}{c||}{clang++}                               \\ [0.5ex]
			\hline
			    & Icelake 8368Q             & EPYC 7763                     & Icelake 8368Q & Ampere A100 \\ [0.5ex]
			\hline\hline
			-O0 & 0.545073\% wrong          & 0.545073\% wrong              & -             & correct     \\ [0.3ex]
			\hline
			-O1 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
			-O2 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
			-O3 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
		\end{tabular}
		\caption{\label{tab:correctness}}
	\end{table}
\end{center}

Further investigation suggests that this behaviour is due to different default semantics for floating point calculations between the two different compilers.
According to Table \ref{tab:semantics} the default model in \texttt{dpcpp} is \texttt{fast} while in \texttt{clang++} is \texttt{precise}.

\begin{center}
	\begin{table}
		\centering
		\begin{tabular}{||ll|l||}
			\hline
			\multirow{4}{*}{dpcpp -O1}          & ---                & 1.20335\% wrong  \\
			                                    & -fp-model=fast     & 1.20335\% wrong  \\
			                                    & -fp-model=strict   & 0.545073\% wrong \\
			                                    & -fp-model=precise  & correct          \\ \hline \hline
			\multirow{4}{*}{clang++ -fsycl -O1} & ---                & correct          \\
			                                    & -ffp-model=fast    & 1.20335\% wrong  \\
			                                    & -ffp-model=strict  & 0.545073\% wrong \\
			                                    & -ffp-model=precise & correct          \\ \hline
		\end{tabular}
		\caption{\label{tab:semantics}}
	\end{table}
\end{center}
