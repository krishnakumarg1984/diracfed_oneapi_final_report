% vim: conceallevel=0:
% chktex-file 1
% chktex-file 44
% chktex-file 13
% chktex-file 31

\documentclass[../main]{subfiles}

\begin{document}

\subsection{OpenQCD}\label{sec:openqcd}

OpenQCD-FASTSUM\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/}} is a code for Lattice QCD simulations.
The main development is in C, with some functions heavily optimized using intrinsics from various instruction sets.
This work builds upon previous DiRAC work, where the code was profiled and the main hotspot was identified as the Dirac-Wilkinson (DW) operator.
The DW operator was ported to CUDA and a data layout optimization was performed to improve performance on V100 GPUs.

\subsubsection{Porting approach}\label{sec:openqcd_porting}

Since the DW operator was identified as the main hotspot in the code and a CUDA port had already been made, our approach was to follow Intel's Recommended Workflow illustrated in Figure~\ref{fig:intel-workflow}, starting from existing CUDA code, and use the Intel DPC++ Compatibility Tool (dpct) to generate DPC++ code from it.

We performed the porting on CSD3.
We found it difficult to work on the Intel DevCloud, since it did not have a CUDA library installed and dpct needs to access the CUDA header files.
Intel cannot provide CUDA headers because of copyright issues.
However, they do have a tutorial on how to use \texttt{dpct} on the DevCloud\footnote{\url{https://www.intel.com/content/www/us/en/developer/articles/training/intel-dpcpp-compatibility-tool-training.html}}.
As a short summary, the user has to provide the CUDA header files.
The tutorial gives instructions on where to place them.

On CSD3 our workflow for dpct was the following:
\begin{enumerate}
	\item Set up modules
	      \begin{enumerate}
		      \item\texttt{module load cuda}
		      \item\texttt{module load gcc}
	      \end{enumerate}
	\item Set up OneAPI environment
	      \begin{enumerate}
		      \item\texttt{source /usr/local/software/intel/oneapi/2022.1/setvars.sh}
	      \end{enumerate}
	\item Run dpct
	      \begin{enumerate}
		      \item\texttt{cd <build directory>}
		      \item\texttt{intercept-build make}
		      \item\texttt{dpct -p compile\_commands.json --gen-build-script}
	      \end{enumerate}
\end{enumerate}
Since the build system used a simple \texttt{Makefile}, we used the \texttt{intercept-build} tool capture the source files and compiler commands from it.
This simplified the use of \texttt{dpct} significantly.
The \verb !--gen-build-script! flag was used to automatically generate a new \texttt{Makefile} for the ported dpc++ code, although the generated \texttt{Makefile} required some manual editing afterwards.
We will describe that in more detail in Section~\ref{sec:openqcd_compiling}.

After running \texttt{dpct} we got a list of warnings where \texttt{dpct} suggests attention is required from the developers.
These are reasonably clear and additional information is available in the documentation.
The warnings are listed in a closed issue in our GitHub repository\footnote{\url{https://github.com/UCL/openqcd-oneapi/issues/20}}.
None of them required our immediate attention to compile and run the code.
From the point of portability, we note that \texttt{dpct} warns us when workgroup sizes are hardcoded for a particular GPU architecture and recommends querying the device for its maximum workgroup size.

\subsubsection{Namespaces}\label{sec:openqcd_namespaces}

The dpc++ code produced by \texttt{dpct} includes a namespace \texttt{dpct} from header files within the OneAPI package.
In our case this was limited to two functions \verb!dpct::atomic_fetch_add()! and \verb!dpct::get_device()!.
In order to make the code portable to other SYCL implementations one needs to replace the references to the (OneAPI specific) \texttt{dpct} namespace with standard \texttt{sycl} functions, or custom code.

The \verb!dpct::atomic_fetch_add()! function is included from the \texttt{atomics.hpp} header file in the OneAPI installation.
The header files are distributed with a fairly permissive Apache-2.0 license with LLVM exceptions that allows to re-use them as long as all the modifications are noted.
Our solution was to include the \verb!dpct::atomic_fetch_add()! function within our source code.
It is a fairly thin wrapper that calls to the \texttt{sycl} namespace.

The \verb!dpct::get_current_device()! function returns a handle to the current device of type \verb #dpct::device_ext#.
In the generated code, this handle is then used to get the default queue object using the \verb #default_queue()# method.
It is not clear why this custom approach has been used by \texttt{dpct}.
The default queue of the current device can be retrieved by standard \texttt{sycl} calls using
\verb !sycl::queue q_ct1{ sycl::default_selector{} }!
which we used to replace the \texttt{dpct} code\footnote{\url{https://github.com/UCL/openqcd-oneapi/pull/23}}.
As an additional benefit, the \texttt{dpct} generated code was retrieving the current device and default queue in each kernel that was allocating or freeing memory on the device, creating unnecessary overhead.
Using standard \texttt{sycl} namespace calls, we were able to create a queue once in the main function, and pass a reference to it to all kernels.

\subsubsection{Compiling}\label{sec:openqcd_compiling}

\paragraph{OneAPI}

To compile the code with OneAPI targeting Intel CPUs, we used the \texttt{dpcpp} compiler included in the OneAPI 2022.1 package.
This worked out of the box using the automatically generated \texttt{Makefile} simply using the \texttt{make} command.

To compile the code with OneAPI targeting NVidia GPUs we had to use Intel's Open Source \verb #clang++# compiler\footnote{\url{https://github.com/intel/llvm}} installed in a private module on CSD3 (\lstinline[basicstyle=\normalsize\ttfamily,breaklines=true,
	breakatwhitespace=false,language=sh]!/usr/local/software/spack/spack-modules/dpcpp-cuda-20220220/linux-centos8-x86_64_v3/!).

In addition to the compiler module, we loaded a recent \verb #gcc/11.2.0# module, and initialised the OneAPI environment, as mentioned in Section~\ref{sec:openqcd_porting}.
In addition to the usual \texttt{Makefile} options, the \verb #-fsycl# and \verb #-fsycl-targets=nvptx64-nvidia-cuda# flags are required to compile SYCL code targeting the NVidia CUDA backend.
The automatically generated \texttt{Makefile} did not work for the NVidia backend, and we  modified it\footnote{\url{https://github.com/UCL/openqcd-oneapi/pull/42}} to simply contain a target of the type
\begin{verbatim}
my_target: $(SRC_FILES)
	$(CC) $(INCLUDES) $(CFLAGS) $^ -o $(EXEC_BASENAME).$@
\end{verbatim}
with the source files listed in \verb #$(SRC_FILES)#, the relevant compiler flags in \verb #$(CFLAGS)# and a path to the header files in \verb #$(INCLUDES)#.

\paragraph{hipSYCL}

As mentioned in section~\ref{sec:software_stack} we have hipSYCL available on CSD3 that can target NVidia A100 GPUs.
It is accessible through the module system with
\begin{verbatim}
module load hipsycl/0.9.2
\end{verbatim}.
Note that we required version 0.9.2, instead of 0.9.1, because support for atomic operations was added in 0.9.2.
Additionally for running a binary produced with hipSYCL it may be necessary to load a module providing newer libstdc++, for example \verb #gcc/9.4.0#
After the modifications to namespaces described in Section~\ref{sec:openqcd_namespaces}, we were able to use it to compile with CPU \texttt{omp} and GPU \texttt{cuda} backends.
One additional code modification was required, The hipSYCL implementation on wraps the \texttt{sycl} namespace inside the \texttt{cl} namespace.
We had to add \verb !using namespace cl;! to the dpcpp source file, but this did not break compatibility with OneAPI.
To target Intel CPUs we added the \verb !--hipsycl-targets=omp! compiler flag, and to target NVidia A100 GPUs we added the \verb !--hipsycl-targets=cuda:sm_80! compiler flag.
Alternatively, one can modify the \verb !HIPSYCL_TARGETS! and \verb !HIPSYCL_GPU_ARCH! environment variables.

As further mentioned in section~\ref{sec:software_stack} it would be possible to install hipSYCL on Cosma8 to run on AMD GPUs.
We made an effort to install it using the \texttt{spack} package manager.
Installing \texttt{spack} is fairly straightforward, all that is required is cloning the GitHub repository\footnote{\url{https://github.com/spack/spack}} and ensuring \texttt{python} is installed.
This can be done in the user's home directory on a HPC system.
To be used effectively, spack requires an environment that points to system packages that should not be reinstalled by the user.
On Cosma8, we used an environment developed by the ExCALIBUR H\&ES project\footnote{\url{https://github.com/ukri-excalibur/excalibur-tests/blob/main/spack-environments/cosma8/compute-node/spack.yaml}}.
Once the environment is set up, we can install hipSYCL with the system-provided version of the \texttt{boost} library and the \texttt{gcc} compiler by
\begin{verbatim}
spack install hipsycl ^boost@1.67.0 %gcc@10.2.0
\end{verbatim}
The installed hipSYCL was successful in compiling for the CPU backend on Cosma8.
However, we discovered that the \texttt{spack} recipe did not contain support for the ROCm backend\footnote{\url{https://github.com/UCL/openqcd-oneapi/issues/29}}, which is necessary in order to compile for AMD GPUs.
The only viable way forward seems for the local sysadmin team to install hipSYCL from source.

\paragraph{ComputeCPP}

ComputeCpp\footnote{\url{https://developer.codeplay.com/products/computecpp/ce/home}} is a SYCL implementation from CodePlay software.
While there are commercial offerings of ComputeCpp from the vendor, there exists a free community edition for the GNU/Linux and Microsoft Windows operating systems.
The ComputeCpp SDK is available for both the x86-64 and AArch64 architectures, which is encouraging, given the increasingly diverse platforms being embraced by modern HPCs. % chktex 8
The free community edition of the software is available from the vendor's developer portal, but requires the creation of a free account and completing an email verification.
Platform-specific downloads are available compressed tarballs, and may be transferred to the relevant HPC system via standard Unix remote file transfer programs such as \texttt{scp},\texttt{sftp} and \texttt{rsync}.
The compiler binaries and other relevant tools shall be located at the root of the uncompressed archive.
The ComputeCpp framework is ably supported by extensive online browser-based documentation, which addresses a variety of developer needs, and provide tutorials on various facets of Sycl programming for heterogeneous systems.
During the course of this project, there were a few releases of the software.

As of June~2022, the ComputeCpp framework complies with the former specification of the SYCL standard \viz~1.2.1, with support for SYCL~2020 marked as experimental.
In particular, the framework did not yet support the SYCL~2020 feature of Unified Shared Memory~(USM) access, in which a pointer on the host is also a valid pointer on the device.
The CUDA version of OpenQCD-FASTSUM, \texttt{dw.cuda}, that was used as the starting point for our SYCL port, relied on the paradigm of explicitly copying the input spinor values to GPU devices, performing the kernel computations and copying the result back to the host.
Following the recommended porting procedure, we used Intel's~\texttt{dpct} tool to obtain the initial SYCL port of the code as discussed in Section~\ref{sec:openqcd_porting}.
The ported code relies heavily on the SYCL analogue of their CUDA \texttt{memcpy} counterpart, making extensive use of~USM for data access.
This SYCL source (with USM pointers) could not be successfully compiled by the latest version of the ComputeCpp compiler.
Until ComputeCpp supports USM access, resolving these compilation errors shall require the SYCL source to be rewritten using the ``buffer-accessor'' memory model, but was not pursued in the interest of time.

\subsubsection{Testing}\label{testing_openqcd}

The repository has a stand-alone test\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/-/tree/feature/cuda_tests/tests/cuda}} for the CUDA port that runs in under a minute on most GPUs and verifies the results of the DW operator against reference data.
Perusing the available documentation, we could not re-generate the reference data using the CPU code\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/-/blob/feature/cuda_tests/tests/cuda/Dump_memory_guide.txt}}.
However, we obtained the original reference data and verified the CUDA code.
The files are temporarily stored in the project directory on CSD3 \texttt{/rds/project/dirac\_vol2/rds-dirac-dr004/openqcd/ref-data}.
Issues pertinent to the perpetual storage of data files required in code testing is discussed in Section~\ref{sec:gitlfs_rds}.

\subsubsection{Research Data Storage and Git LFS}\label{sec:gitlfs_rds}

The reference data consists of a set of non-text \ie~binary files which consists of filesystem hex dumps of various objects in memory instrumented both before and after the \texttt{dw} function call.
This necessitates that their storage and retrieval be separate from, yet be seamlessly available with, the rest of the source files in the repository.
Such binary blobs are not amenable to version control since the classical Unix/git tools such as \texttt{diff} cannot operate on non-text files.
Furthermore, depending on the specific lattice grid sizes used, the storage requirements of these data files range from a few megabytes to tens of gigabytes on the filesystem.

Owing to its ability to handle large non-text files seamlessly in the same code repository, we chose the Git Large File Storage~(LFS) service\footnote{\url{https://git-lfs.github.com/}} for our data storage needs.
One attractive proposition of the LFS service is that typical git operations such as fetch and push can also be applied to the data files by using the \texttt{lfs} subcommand.
When pushed up to the repository, Git~LFS converts such binary files into plain-text files that merely hold pointers to the actual data location held elsewhere, typically within an institution's LFS storage allocation.
When these data files are fetched from the remote repository onto a local clone on the user's filesystem, the actual files are seamlessly retrieved using the pointer information.
While in principle, this approach works well, \eg~in the scenario of a solo developer with one local clone of a repository, in practice it is fraught with a myriad of practical difficulties, which we discuss here.

Firstly, the LFS subcommand needs to be available in user's \textsc{PATH}.
On a remote HPC machine where a standard user does not typically have administrative privileges to install software, the LFS service needs to be made available through the linux modules~(LMod) system.
While the CSD3 cluster at Cambridge does indeed have a user module for git lfs, this is indeed a practical difficulty on other remote HPCs where it is not always available or easily installable on the user's home directory.

Secondly, for each local clone, Git~Lfs requires the initialisation of user hooks explicitly by issuing an suitable invocation command, ideally prior to the cloning of the remote repository.
This presents numerous difficulties in a collaborative scenario, wherein some users might not yet have initialised the hooks.
In this scenario, a fetch operation from upstream shall have silent side effects \eg~indicate the availability of expected file names in the correct paths, but resulting in runtime failures and segmentation faults since these files merely contain pointer information and not the actual binary data.
This leads to wasted developer time in debugging.

Thirdly, without careful attention to the lfs subcommands and appropriate filter hooks in place (especially when cloning and working from a new filesystem location or machine), there is a strong probability of one of the developers accidentally committing the large binary files to the repository.
This consumes the quota for data storage allocation of an organisation, and leads to prohibitive times for each standard git operation, thereby adversely affecting development.
With Github's current design, the only remedy for these problems is to manually migrate to a new repository and thereafter delete the existing repository.
While commit history can be automatically migrated, it requires tedious manual intervention to migrate bug reports, the Kanban board and project wiki, while closed pull requests and associated notes/discussions are lost forever.
In view of the aforesaid difficulties which we faced during this project, it is recommended to seek alternatives for long-term data storage needs for the reference data files.

\subsubsection{Continuous Integration}

It is possible to install prebuilt binaries of both OneAPI and hipSYCL compilers from public repositories relatively quickly, which enabled us to use CI workflows during development.
CI helps catch errors quickly, and ensure that code merged into the main development branch successfully builds and executes test cases.
It is standard practice in industry and we warmly recommend it for scientific software projects.
We chose to use GitHub Actions as the CI platform of our choice, due to good integration with the GitHub source repository and relatively friendly pricing rules, which allow us to run our CI jobs free of charge.
In GitHub Actions, the user defines a workflow in a \verb #.yaml# file which can consist of any number of arbitrary steps.
We implemented a simple workflow that builds and runs the main executable on two data sets, using the OneAPI\footnote{\url{https://github.com/UCL/openqcd-oneapi/blob/e5d53c7e47d8b94f78c87b91ef218d217ce57597/.github/workflows/oneapi.yml}} and hipSYCL\footnote{\url{https://github.com/UCL/openqcd-oneapi/blob/e5d53c7e47d8b94f78c87b91ef218d217ce57597/.github/workflows/hipsycl.yml}} compilers, on a CPU-only machine running the latest Ubuntu version available.
It was not possible to test the GPU builds at this time.
To install the OneAPI compiler, we followed this blog\footnote{\url{https://neelravi.com/post/oneapi-github-workflow/}} and Intel's instructions\footnote{\oneapiaptinstall}.
Installing the whole OneAPI package would take too long for the CI job, the main issue was finding the right packages for installing the \verb #dpcpp# compiler only.
To install hipSYCL, we used instructions for installing from repositories from the developers\footnote{\hipsyclinstallfromrepos}.

\subsubsection{Performance}\label{sec:openqcd_performance}

To measure the performance on the NVidia A100 GPUs available on CSD3, we profiled the CUDA and the SYCL versions of the code with the NVidia NSight Compute profiler \texttt{ncu}.
NVidia tools seemed the only feasible option for profiling performance on NVidia devices.

We found that NSight compute works reasonably well on SYCL code.
One issue is that, since the kernels executed on the GPU are generated from lambda functions in SYCL, function names are not retained as kernel names.
Instead, the profiled kernels have generic names that can be hard to decipher if the code contains multiple kernels.
To avoid this issue, it is recommended to name the kernels using a class passed as a template argument to \verb #parallel_for#, \eg~\lstinline{q.parallel_for<class name_of_my_kernel>(...)}.
Note, that the names given to kernels have to be unique, i.e.\ not clash with names of functions or other code constructs.

The main profiling results are summarised in Table~\ref{tab:openqcd_perf}.
We have profiled the three main kernels \texttt{mulpauli}, \texttt{doe} and \texttt{deo}.
The majority of the code is performing multiply and add operations of complex data types stored in nested data structures.
The original CUDA code clearly outperforms both OneAPI and hipSYCL ports.
The difference in performance varies from roughly a factor 2 in the \texttt{doe} kernel, to a factor 6 in the \texttt{deo} kernel.

A more detailed look into the profiling data shows that the main performance bottleneck is the memory throughput.
The CUDA code uses the GPU global memory exclusively, whereas the SYCL ports are moving data in and out of both local and global memory.
This extra memory movement results in a lower Arithmetic Intensity, reported by the roofline analysis in \texttt{ncu}, in the range of $0.3 - 0.7$ for the SYCL ports, while the CUDA kernels have an Arithmetic Intensity close to $1.2$.
Our interpretation is that the data movement to local memory is due to the SYCL ports using a higher number of registers per thread, as can be seen in Table~\ref{tab:openqcd_perf}.
If the registers of the GPU get oversubscribed, the data spills over to local memory that gets used as a cache.
The extra data movement leads to a lower theoretical maximum occupancy of the GPU threads, with \texttt{ncu} reporting maximum occupancies of $65$~\%, $25$~\% and $25$~\% in the three main kernels for the CUDA code, $25$~\%, $18.75$~\% and $18.75$~\% for OneAPI and $18.75$~\%, $12.5$~\% and $18.75$~\% for hipSYCL, respectively.
The achieved occupancies are reported in Table~\ref{tab:openqcd_perf} and follow the theoretical maxima very closely.

The kernels are using a workgroup size of 128.
We tried varying the workgroup size of the kernels between 64 and 256, but did not find any significant difference in performance.
In order to improve the performance of the SYCL code, our conclusion is the register use issue should be resolved, in order to raise the occupancy and eliminate the local memory usage.
It is worth noting that the two compilers we used generate a different number of registers per thread from the same source code.
It seems plausible that there is a deficiency in the current state of the SYCL compilers, rather than the source code itself.
This could be a good topic for investigation in future projects or hackathons.

% Based on our initial comparisons, the CUDA kernels in OpenQCD perform better than the SYCL kernels we generated.
% We have summarized the preliminary results in table~\ref{tab:openqcd_perf}.
% The difference varies between the kernels we studied, from a factor of 2 to a factor of 6.
% We note that the compute throughput seems categorically lower by roughly a factor of 2, while the memory throughput is comparable in the \verb #mulpauli# and \verb #doe# kernels and lower by a factor of 2 in the \verb #deo# kernel.
% NSight Compute suggests the kernels are bottlenecked by memory throughput.
% A significant difference between the memory use of the CUDA and SYCL kernels is the CUDA kernels are only using Global memory while SYCL kernels are using both Global and Local memory.
% The roofline analysis gives an AI of 0.37 for the SYCL code and 1.15 for the CUDA code.
% It seems safe to say that the SYCL code is performing memory accesses that may be unnecessary.
% This requires further study and optimization.
% \todo{@Tuomas Add what we found.Occupancy vs register pressure seems to be the main indicator} Changes to workgroup size did not have a significant effect on performance on NVidia A100s.

\begin{table}[!htbp]
	\begin{tabular}{@{}l l S S S S S@{}}
		\toprule
		\thead{Kernel                                                                         \\ \vphantom{(ms)}} &
		\thead{Compiler                                                                       \\ \vphantom{(ms)}} &
		{\thead{Duration                                                                      \\ (ms)}} &
		{\thead{Compute                                                                       \\ throughput (\%)}} &
		{\thead{Memory                                                                        \\ throughput (\%)}} &
		{\thead{Occupancy                                                                     \\ (\%)}} &
		{\thead{Registers                                                                     \\ per thread}} \\
		\midrule
		\multirow[t]{2}{*}{\texttt{mulpauli}} & CUDA    & 4.85  & 16.05 & 81.28 & 60.09 & 48  \\
		                                      & OneAPI  & 17.21 & 7.39  & 70.98 & 23.30 & 116 \\
		                                      & hipSYCL & 8.12  & 10.34 & 78,51 & 17.60 & 168 \\
		\cmidrule{1-7}
		\multirow[t]{2}{*}{doe}               & CUDA    & 7.57  & 15.78 & 62.59 & 24.65 & 116 \\
		                                      & OneAPI  & 17.41 & 7.31  & 61.21 & 18.44 & 168 \\
		                                      & hipSYCL & 11.14 & 10.57 & 63.54 & 12.28 & 255 \\
		\cmidrule{1-7}
		\multirow[t]{2}{*}{deo}               & CUDA    & 8.24  & 12.94 & 54.93 & 23.92 & 108 \\
		                                      & OneAPI  & 46.32 & 7.64  & 27.22 & 18.54 & 142 \\
		                                      & hipSYCL & 54.90 & 4.80  & 30.96 & 18.60 & 168 \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:openqcd_perf}Performance comparison of CUDA and SYCL OpenQCD DW kernels on NVidia A100.}
\end{table}

\subsubsection{Accessing Device and Host Memory}\label{sec:openqcd_memoryaccess}

The SYCL~2020 standard supports two main modes of managing host/device memory.
The first mode of memory management is Unified Shared Memory~(USM) in which is a pointer on the host is also a valid pointer on the device.
The USM pointers may simply be dereferenced to access the underlying data either on the host or on the device as needed.
USM offers a convenient abstraction to data transfer paradigms pertinent to heterogeneous systems, and offers convenient syntactic sugar to relieve the developer of the bookkeeping of pointer ownership, validity and other low-level memory access aspects.

SYCL also supports the `buffer-accessor` model of memory management introduced prior to the SYCL~2020 standard.
Here, a buffer object is declared as container references to underlying data, with the constraint that neither the host or the device may directly access the buffer.
Instead, accessor objects are created on the host/device to obtain access to the required data.
Although the default accessor objects are created with read and write capabilities, it is possible to optimise this by declaring read-only or write-only accessor objects as per the task at hand.

For SYCL ports of existing C++ codes that makes extensive use of pointers, the USM approach is regarded as a natural starting point.
Indeed the output of the \texttt{dpct} transpiler produced USM SYCL code from the CUDA code.
Within USM programming, there exist two paradigms --- 1) Implicit USM access and 2) Explicit USM access.
Since the input CUDA code has explicit copying of data between the host and device, the SYCL code produced by the transpiler also included the SYCL analogues \ie~used explicit USM access.
After noting the inferior performance of the generated explicit-USM SYCL code as reported in table~\ref{tab:openqcd_perf}, we adapted the code to use implicit memory access with a view to investigate its performance.

In implicit USM, there are no explicit function calls to copy data between host and device, and there exists three modes of allocating memory --- host, device and shared.
In host memory allocation, the memory allocated on the host can be accessed on-demand on the device.
However, a key consideration is that this does not place the data on the device's own memory, but rather is fetched on demand over a slower PCIe bus.
This may be suitable if one-off access to moderate-volume data is needed.
In device allocation, memory is allocated on the device and cannot be accessed directly on the host.
In shared allocation, the runtime is free to perform data transfers between the host and device memories as per the kernel's data requirements.

For the spinors \texttt{m,n,u,piup,pidn}, we used host allocation since the device kernels do not require repeated access to the data.
Furthermore, this enabled us to clean up the SYCL code by removing the \texttt{malloc} lines corresponding to explicit USM access.
For the spinor \texttt{r}, \ie~the main output of the \texttt{dw} computation, a shared allocation was chosen, since this result is used in the invoking function.
After adapting the SYCL code to use implicit USM, performance analysis did not reveal any significant gains over that reported in table~\ref{tab:openqcd_perf}.
However, it should be noted that the CUDA and SYCL codes currently incur a bookkeeping overhead of data structure realignment from Array-of-Structures to Structure-of-Arrays for performance reasons.
In the future, it is intended that the maintainers of OpenQCD shall perform this data structure rearrangement in their codebase, thus enabling us to remove the relevant lines.
Once this change is incorporated in upstream, the performance of the implicit USM SYCL codes shall be revisited.

\subsubsection{Estimate of Development Effort}\label{sec:openqcd_personhours}

We had two developers (TK and KG) working on openqcd for most of the project duration.
This strategy helps us routinely use good software development practices, such as pair programming and code review.
The planned time allocation was 50~\% of time from both developers, however due to unforeseen personal circumstances we were not able to deliver all of the allocated time.
At the start of the project, both developers were proficient c++ programmers, with varying knowledge of C, CUDA, and SYCL, but lacked familiarity with the openqcd codebase.
As a rough estimate, of the six months spent on the project, the first two were spent getting familiar with the code, the Intel tools, and the programming languages.
The first meaningful commits to the repository were made two months after the start of the project\footnote{\url{https://github.com/UCL/openqcd-oneapi/commits/arc_dev}}, with fairly rapid development in the following month.
We spent a total of approximately 330 working hours of development time within the six month period.
This includes the development time, but also the time spent going through tutorials, meetings with the team, reading textbooks, planning and design.
In our experience, with the automatic translation tool \texttt{dpct}, it is quite straightforward to port pre-existing CUDA code into a SYCL code that will compile with OneAPI and run successfully on CPU and GPU hardware and the tool is accessible to even programmers who are SYCL novices.
We estimate that most of the development effort went into making the code portable across different SYCL compilers, setting up the testing infrastructure and analysing the performance.

\end{document}
