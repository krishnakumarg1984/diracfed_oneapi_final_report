%!TEX root = ../main.tex
% chktex-file 1
% chktex-file 44
% chktex-file 13

\subsection{OpenQCD}\label{sec:openqcd}

OpenQCD-FASTSUM\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/}} is a code for Lattice QCD simulations.
The main development is in C, with some functions heavily optimized using intrinsics from various instruction sets.
This work builds upon previous DiRAC work, where the code was profiled and the main hotspot was identified as the Dirac-Wilkinson (DW) operator.
The DW operator was ported to CUDA and a data layout optimization was performed to improve performance on V100 GPUs.

\subsubsection{Porting approach}\label{sec:openqcd_porting}

Since the DW operator was identified as the main hotspot in the code and a CUDA port had already been made, our approach was to follow Intel's Recommended Workflow illustrated in Figure~\ref{fig:intel-workflow}, starting from existing CUDA code, and use the Intel DPC++ Compatibility Tool (dpct) to generate DPC++ code from it.

We performed the porting on CSD3.
We found it difficult to work on the Intel DevCloud, since it did not have a CUDA library installed and dpct needs to access the CUDA header files.\todo{mention Intel tutorial on how to use dpct on DevCloud}Based on examples we found, others had worked around this by uploading their own CUDA headers to their DevCloud account, but this seemed too cumbersome.

On CSD3 our workflow for dpct was the following:

\begin{enumerate}
	\item Set up modules
	      \begin{enumerate}
		      \item\texttt{module load cuda}
		      \item\texttt{module load gcc}
	      \end{enumerate}
	\item Set up OneAPI environment
	      \begin{enumerate}
		      \item\texttt{source /usr/local/software/intel/oneapi/2022.1/setvars.sh}
	      \end{enumerate}
	\item Run dpct
	      \begin{enumerate}
		      \item\texttt{cd <build directory>}
		      \item\texttt{intercept-build make}
		      \item\texttt{dpct -p compile\_commands.json --gen-build-script}
	      \end{enumerate}
\end{enumerate}

Since the build system used a simple Makefile, we used the intercept-build tool capture the source files and compiler commands from it.
This simplified the use of dpct significantly.
The --gen-build-script flag was used to automatically generate a new Makefile for the ported dpc++ code, although the generated Makefile required some manual editing afterwards.

After running dpct we got a list of warnings where dpct suggests attention is required from the developers.
These are reasonably clear and additional information is available in the documentation.
The warnings are listed in a closed issue on the GitHub repository\footnote{\url{https://github.com/UCL/openqcd-oneapi/issues/20}}.
None of them required our immediate attention to compile and run the code.
From the point of portability, we note that dpct warns us when workgroup sizes are hardcoded for a particular GPU architecture and recommends querying the device for its maximum workgroup size.

\subsubsection{Namespaces}\label{sec:openqcd_namespaces}

The dpc++ code produced by dpct includes a namespace \texttt{dpct} from header files within the OneAPI package.
In our case this was limited to two functions \verb!dpct::atomic_fetch_add()! and \verb!{dpct::get_device()!.
In order to make the code portable to other SYCL implementations one would need to remove the references to the (OneAPI specific) dpct namespace.

The \verb!dpct::atomic_fetch_add()! function is included from the \texttt{atomics.hpp} header file.
The header files are distributed with a fairly permissive Apache-2.0 license with LLVM exceptions that allows to re-use them as long as all the modifications are noted.
Our solution was to include the \verb!dpct::atomic_fetch_add()! function within our source code.
It is a fairly thin wrapper that calls to the \texttt{sycl} namespace.

The \verb!dpct::get_device()! function seems to differ in no way in functionality from the function of the same name in the \texttt{sycl} namespace and can be replaced with no obvious issues.\todo{@Tuomas Report on the porting of getdevice calls}

\subsubsection{Compiling}\label{sec:openqcd_compiling}

To compile the code targeting Intel CPUs, we used the \texttt{dpcpp} compiler included in the OneAPI 2022.1 package.
This worked out of the box using the automatically generated Makefile simply using the \texttt{make} command.

To compile the code targeting NVidia GPUs we had to use Intel's Open Source \verb #clang++# compiler\footnote{\url{https://github.com/intel/llvm}} installed in a private module (\verb #/usr/local/software/spack/spack-modules/dpcpp-cuda-20220220/linux-centos8-x86_64_v3/# ) on CSD3.

In addition to the compiler module, we had to load a recent \verb #gcc/11.2.0# module, and initialise the OneAPI environemnt, as mentioned in Section~\ref{sec:openqcd_porting}.
In addition to the usual Makefile options, one has to pass the flags \verb #-fsycl# and \verb #-fsycl-targets=nvptx64-nvidia-cuda# to compile sycl code targeting the NVidia CUDA backend.
The automatically generated Makefile did not work for the NVidia backend, and we manually modified it\footnote{\url{https://github.com/UCL/openqcd-oneapi/pull/42}} to simply contain a target of the type
\begin{verbatim}
my_target: $(SRC_FILES)
	$(CC) $(INCLUDES) $(CFLAGS) $^ -o $(EXEC_BASENAME).$@
\end{verbatim}
with the source files listed in \verb #$SRC_FILES#, the relevant compiler flags in \verb #$(CFLAGS)# and a path to the header files in \verb #$(INCLUDES)#.

\todo{@Tuomas Add compiling with HIPSycl}
\todo{@Krishna Add (issues with) compiling with ComputeCPP}
\todo{@Tuomas Add (issues with) compiling on AMD}

\subsubsection{Testing}\label{testing_openqcd}

The repository contains a stand-alone test\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/-/tree/feature/cuda_tests/tests/cuda}} for the CUDA port that runs in less than a minute on most GPUs and verifies the results of the DW operator against reference data.
We found generating the reference data difficult using the main CPU code based on\footnote{\url{https://gitlab.com/fastsum/openqcd-fastsum/-/blob/feature/cuda_tests/tests/cuda/Dump\_memory\_guide.txt}}.
However, we were able to recover the original reference data and verify the CUDA code.
The files are temporarily stored in the project directory on CSD3 \texttt{/rds/project/dirac\_vol2/rds-dirac-dr004/openqcd/makis-ref-data}.
This is not a very sustainable solution however, and should be moved to, e.g. GitHub LFS or a similar data storage service.\todo{@Krishna, Report on difficulties with Git LFS on GitHub}

\todo{@Tuomas report on CI}

\subsubsection{Continuous Integration}

It is possible to install prebuilt binaries of both OneAPI and HipSYCL compilers from repositories relatively quickly, which enables us to use CI workflows during the development work. CI helps catch errors quickly, and ensure that code merged into the main development branch successfully builds and executes test cases. It is standard practice in indurstry and we warmly recommend it for scientific software projects. We chose to use GitHub Actions as the CI platform of our choice, due to good integration with the GitHub source repository and relatively friendly pricing rules, which allow us to run our CI jobs free of charge. In GitHub Actions, the user defines a workflow in a \verb #.yaml# file which can consist of any number of arbitrary steps. We implemented a simple workflow that runs the main executable on two data sets, using the OneAPI\footnote{\url{https://github.com/UCL/openqcd-oneapi/blob/e5d53c7e47d8b94f78c87b91ef218d217ce57597/.github/workflows/oneapi.yml}} and HIPSYCL\footnote{\url{https://github.com/UCL/openqcd-oneapi/blob/e5d53c7e47d8b94f78c87b91ef218d217ce57597/.github/workflows/hipsycl.yml}} compilers, on a CPU-only machine running the latest Ubuntu version available. It was not possible to test the GPU builds at this time. To install the OneAPI compiler, we followed this blog\footnote{\url{https://neelravi.com/post/oneapi-github-workflow/}} and Intel's instructions\footnote{\url{https://www.intel.com/content/www/us/en/develop/documentation/installation-guide-for-intel-oneapi-toolkits-linux/top/installation/install-using-package-managers/apt.html#apt}}. Installing the whole OneAPI package would take too long for the CI job, the main issue was finding the right packages for installing the \verb #dpcpp# compiler only. To install HIPSycl, we used instructions for installing from repositories from the developers\footnote{\url{https://github.com/illuhad/hipSYCL/blob/develop/install/scripts/README.md#installing-from-repositories}}.

\subsubsection{Performance}\label{sec:openqcd_performance}

To measure the performance on the NVidia A100 GPUs available on CSD3, we profiled the CUDA and the SYCL versions of the code with the NVidia NSight Compute profiler.
NVidia tools seemed the only feasible option for profiling performance on NVidia devices.

We found NSight compute works reasonably well on SYCL code.
The main issue is that since the kernels executed on the GPU are generated from lambda functions in SYCL, the function names are not retained as kernel names.
Instead, the profiled kernels have generic names that can be hard to decipher if the code contains multiple kernels.
To avoid this issue, it is recommended to name the kernels using a class passed as a template argument to \verb #parallel_for#, e.g.
\begin{verbatim}
    q.parallel_for<class name_of_my_kernel>(...)
\end{verbatim}
Note, that the names given to kernels have to be unique, ie. not clash with names of functions or other code constructs.

Based on our initial comparisons, the CUDA kernels in OpenQCD perform better than the SYCL kernels we generated.
We have summarized the preliminary results in table~\ref{tab:openqcd_perf}.
The difference varies between the kernels we studied, from a factor of 2 to a factor of 6.
We note that the compute throughput seems categorically lower by roughly a factor of 2, while the memory throughput is comparable in the \verb #mulpauli# and \verb #doe# kernels and lower by a factor of 2 in the \verb #deo# kernel.
NSight Compute suggests the kernels are bottlenecked by memory throughput.
A significant difference between the memory use of the CUDA and SYCL kernels is the CUDA kernels are only using Global memory while SYCL kernels are using both Global and Local memory.
The roofline analysis gives an AI of 0.37 for the SYCL code and 1.15 for the CUDA code.
It seems safe to say that the SYCL code is performing memory accesses that may be unnecessary.
This requires further study and optimization.\todo{@Tuomas Add what we found. Occupancy vs register pressure seems to be the main indicator}

\subsubsection{Accessing Device and Host Memory}\label{sec:openqcd_memoryaccess}

\todo{@Krishna Report on Explicit vs Implicit memory movement in USM, mention buffer/accessor model}

\begin{center}
	\begin{table}
		\begin{tabular}{||c c c c c||}
			\hline
			Kernel Name & Language & Duration [ms] & Compute Throughput [\%] & Memory Throughput [\%] \\
			\hline\hline
			mulpauli    & CUDA     & 4.85          & 16.07                   & 81.30                  \\
			\hline
			mulpauli    & SYCL     & 17.31         & 7.36                    & 70.89                  \\
			\hline
			doe         & CUDA     & 7.57          & 15.78                   & 62.66                  \\
			\hline
			doe         & SYCL     & 17.34         & 7.31                    & 61.12                  \\
			\hline
			deo         & CUDA     & 8.25          & 12.93                   & 54.88                  \\
			\hline
			deo         & SYCL     & 46.30         & 7.66                    & 27.24                  \\
			\hline
		\end{tabular}
		\caption{\label{tab:openqcd_perf}Performance comparison of CUDA and SYCL OpenQCD DW kernels.}
	\end{table}
\end{center}

\todo{@Tuomas Update table with HIPSycl performance on NVidia A100's}

\subsubsection{Ideas for Future Work}\label{sec:openqcd_future_work}

Here we give a brief summary of interesting topics that could be investigated further within the rest of this project.

\begin{itemize}
	\item The workgroup size used by the kernels was hand-tuned to 128 for the V100 architecture. In SYCL we can query the device for its maximum workgroup size so it might be possible to make this portable by automatically detecting a suitable size.\todo{Still TODO}
	\item The profiling with \verb #ncu# suggests that there is unnecessary memory movement in the kernels generated in the SYCL code. Identifying and eliminating them seems like the most obvious path to getting closer to the CUDA performance.\todo{See comments above regarding occupancy}
	\item The porting approach with \verb #dpct# was remarkably successful in this case. It would be interesting to see if we can port more kernels with the same level of effort.\todo{Outside scope of this project}
	\item Compile with hipSYCL and ComputeCpp and compare performance.\todo{See notes in compiling.}
	\item Both the CUDA and SYCL ports are restricted to two test cases and can't be used in the production code at the moment. For the research team, it would be useful if these ports could be upstreamed to the main development.\todo{Not done within this project but could be in scope for future work by DiRAC. Add our advice on upstreaming strategy.}
	\item The correctness of the code is verified by comparing the final result of the computation against reference data. There would be scope to add unit tests to test the code more thoroughly.\todo{See notes in testing}
	\item A sustainable solution for the storage and access of the reference data sets needs to be found and implemented. \todo{See notes in testing}
\end{itemize}

\subsubsection{Estimate of Development Effort}\label{sec:openqcd_personhours}
