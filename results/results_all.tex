%!TEX root = ../main.tex

\input{results/openqcd_main}
\input{results/openmm_main}

\subsection{HemeLB}
Computational fluid dynamics represents a significant use case for high performance computing in many fields of engineering and science. Of the many techniques available in this field, the lattice Boltzmann method (LBM) has gained traction in the community for its ability to be both simply deployed for parallel computation and its facility to handle complex geometric and moving boundary conditions. The HemeLB code \footnote{\url{https://github.com/hemelb-codes}} uses the LBM to study 3D blood flow in human-scale vascular domains and has been optimised to handle the sparse geometries characteristic of these problems. In previous work, the CPU version of the code has been shown to display strong scaling to over 300,000 cores on SuperMUC-NG (\textbf{citation}) whilst a GPU port of the code has displayed similar characteristics to over 18,000 NVIDIA V100 GPUs on Summit \cite{zacharoudiou_development_2022}.

The current GPU ports of HemeLB have been developed using the CUDA framework. Whilst this has been suitable for the current generations of HPC infrastructure, the imminent arrival of alternative accelerator hardware has necessitated the implementation of more platform-agnostic coding structures. This effort porting to the OneAPI platform will be instructive as to the level of complexity of such a migration for a mature, detailed and highly performant application.

The OneAPI platform advocates the use of its DPCT conversion tool to convert the majority of an existing CUDA code to the DPC++ framework. From a practical standpoint, it can be noted that the DevCloud environment that is offered for DPC++ development does not appear to natively contain the CUDA header files necessary for the conversion of an existing CUDA code with DPCT to take place. As such, conversion needs to be done locally or on a cluster that contains both CUDA and OneAPI software. Additionally, the majority of resources on the use of DPCT focus on simple, single file, examples of CUDA code. In reality, porting a mature code will require the conversion and correction of several files to ensure a fully operational application is able to be compiled. This is further complicated by features such as MPI communication between multiple GPUs. For this current exercise, we have extracted a single HemeLB collision kernel to test the performance variation between a native CUDA implementation and that converted to DPC++. The chosen kernel computes the single-relaxation time collision function used by the LBM to solve fluid flow within the simulation domain. This is used to update the vast majority of sites at each time step and represents the bulk of computation within an iteration.

This kernel was extracted from the main HemeLB code and written into a single CUDA file with appropriate supporting structures and data to allow it to run on a single GPU. This, highly simplified, script was converted to DPC++ with DPCT. On our first attempt, the conversion yielded one error that needed to be corrected by hand. Here it failed to convert a declaration of a constant memory array despite successfully converting several around the same location:

\begin{lstlisting}[language=C++,basicstyle=\small]
dpct::constant_memory<int, 1> _InvDirections_19(19); <--- Converted successfully
//__constant__ double _EQMWEIGHTS_19[19]; <--- Remained after DPCT conversion
dpct::constant_memory<double, 1> _EQMWEIGHTS_19(19); <--- Manually added
dpct::constant_memory<int, 1> _CX_19(19); <--- Converted successfully
\end{lstlisting}

For a updated version of the script, this error no longer occurred.

\subsubsection{Performance Analysis - Profiling}
Following the conversion, we compared the performance of the native CUDA code and the converted code on the NVIDIA A100 GPUs available on the CSD3 cluster. We also compared this to the Intel GPUs available on the Intel DevCloud environment. This hardware however should not be regarded as comparable to the A100 cards and these results demonstrate the capacity for cross-platform deployment. For this test we ran the kernel for the equivalent of 5000 iterations of a 100,000 site domain. This was repeated 10 times to obtain a fair estimate of the runtime.

\begin{center}
	\begin{tabular}{||c c c||}
		\hline
		Hardware    & Code  & Average Runtime [s]     \\ [0.5ex]
		\hline\hline
		NVIDIA A100 & CUDA  & 0.398578 +/- 0.00020542 \\
		\hline
		NVIDIA A100 & DPC++ & 0.436205 +/- 0.00793029 \\
		\hline
		Intel P630  & DPC++ & 15.4501 +/- 0.0939887   \\ [1ex]
		\hline
	\end{tabular}
\end{center}

For this initial test case, it can be seen that on the common hardware tested here the conversion to the DPC++ framework and clang compiler has resulted in a 10\% drop in performance compared to the native CUDA code and NVIDIA compiler.

\begin{figure}[htp]
	\centering
	\includegraphics[clip,width=\textwidth]{Nsight_Compute_CUDA_vs_DPC++_A100_GPU.png}
	\caption{Profiling the CUDA and ported DPC++ code using NVIDIA's Nsight Compute on A100 GPU.}
	\label{fig:ncu_CUDA_Vs_DPC++_A100GPU}
\end{figure}

Profiling both versions of the code (CUDA, DPC++) was carried out using NVIDIA's performance analysis tools, Nsight Compute (see fig.~\ref{fig:ncu_CUDA_Vs_DPC++_A100GPU}) and Nsight Systems (see fig.~\ref{fig:nsys_DPC++_A100GPU}). Nsight Compute provides detailed performance metrics at the kernel level and enables comparison of both versions, although we are not certain about the reliability of the results reported for the DPC++ code. Nsight Compute reports a 37\% reduction of the compute throughput and a 58\% increase of the kernel's execution time compared to CUDA, while a $\sim10\%$ reduction in overall computational time was reported using either CPU timers (chrono library) or CUDA-specific timers (CUDA event API).
One issue we came across was that the profiler did not return the kernel's name for the DPC++ code. This information was deducted from the launch statistics (gridsize and blocksize) that matched the corresponding values from profiling the native CUDA code. The DPC++ ported function (CUDA lernel) is labelled as 'const:', instead of the proper kernel's name.


Similar issues are observed with Nsight Systems. Furthermore, the memory transactions to/from the GPU global memory are reported as memory copies to/from the Host.


\begin{figure}[htp]
	\centering
	\includegraphics[clip,width=\textwidth]{profile_nsys_DPC++_A100_2steps.png}
	\caption{Profiling the ported DPC++ code using NVIDIA's Nsight Systems on A100 GPU.}
	\label{fig:nsys_DPC++_A100GPU}
\end{figure}

In this porting exercise, we have only examined HemeLB's main fluid collision kernel. With this proof-of-concept completed, the next step would be to work through the full version of the HemeLB code to fully port it to the DPC++ format. Being a mature code, conducting such a project port will more strongly test the capabilities to effectively port MPI-parallelised CUDA code to the OneAPI framework.

\subsection{dGpoly3D}\label{dgpoly3d}
Discontinuous Galerkin (dG) methods on meshes consisting of polytopic elements have received considerable attention in recent years. By combining advantages from both finite element methods (FEMs) and finite volume methods (FVMs) they allow the simple treatment of complicated computational geometries, ease of adaptivity and stability for non-self-adjoint PDE problems. It has also been shown that they are applicable on extremely general meshes, consisting of general polytopic elements with an arbitrary number of faces and different local elemental polynomial degrees. A basic feature of these methods is the use of physical frame polynomial bases and this, together with the highly involved quadrature requirements over polytopic elements, pose new algorithmic challenges in the context of matrix assembly. The implementation of arbitrary order quadrature rules over polytopic domains is non-trivial with the most general and widely used approach being the subdivision of polytopic elements into basic simplicial sub-elements; standard quadrature rules are then employed on each sub-element. \texttt{dgpoly3d} \cite{dong_gpu-accelerated_2021} is a CUDA implementation of the symmetric interior penalty dG method, for the linear system assembly step of second order advection-diffusion-reaction equations.

The code first subdivides each 3D polytopic element into simplicial sub-elements (tetrahedrons) and any co-hyperplanar 2D faces into simplicial sub-faces (triangles), it then precomputes the sparsity pattern of the stiffness and mass matrix and stores it in CSR format and finally populates the matrix with the quadrature values for each simplex of the simplicial subdivision. Different kernels are called for each term of the bilinear form, since the workload of each term can be substantially different. For example, the kernel that calculates the integral over the elements needs a 3D quadrature rule, compared to the kernel over the interior faces which needs a 2D quadrature rule and thus less integration points. For this project we will port one of the kernels, the kernel over the elemental integrals. This kernel spawns as many threads as there are tetrahedrons on the mesh and for each tetrahedron it computes the contribution of this simplex and writes it into the main block diagonal of the matrix using atomic operations. Atomic operations are needed since threads with contiguous thread id's might belong in the same polyhedron and thus will try to update the same memory location simultaneously.

\subsubsection{Preparing the code}
The code is a combination of Python scripts for the non computationally demanding parts along with native CUDA kernels for the matrix assembly process, called from within Python with the PyCUDA module. The code was executed for different mesh sizes and with $p=2$ (the degree of the polynomial on every polyhedron) and input and output data to and from the kernel were captured and stored, both to be able to execute the kernel without having to do the initialization steps every time but also to be used in a unit test.

Since Intel's DPC++ Compatibility tool would not be able to port the CUDA API calls (written in PyCUDA), a minimal C++ code to drive the kernel was written. This code reads the input and output data from disk, performs the memory transfers from and to the device, calls the kernel and finally the unit test. The unit test checks if the matrix entries match within a tolerance with the original values and for this a C++ version of Python's float comparison \texttt{math.isclose()} function was written, that takes as parameters both a relative and an absolute tolerance and returns True if

\begin{verbatim}
abs(x - y) <= max(rel_tol * max(abs(x), abs(y)), abs_tol))
\end{verbatim}
otherwise it returns False. For the following comparisons we've set the relative tolerance to $1\%$ and the absolute to $10^{-5}$.

\subsubsection{Porting the code}
We ported the code using Intel's DPC++ Compatibility tool (\texttt{dpct}). Porting completed without errors and the compatibility tool added inline comments to explain some of its decisions or even propose changes (for example in one place where we access with atomic operations an array residing in global memory, it proposed a change in case the array is in local memory instead). We also automatically created a \texttt{Makefile} with the \texttt{--gen-build-script} flag. However, compiling failed with the following error message:
\begin{verbatim}
  no known conversion from 'dpct::accessor<float,
  dpct::constant, 2>' 'float (*)[3]' for 1st argument
\end{verbatim}
The problem was found to be with the function call that evaluates the Legendre polynomials at a given point. In the CUDA code, the coefficients of the Legendre polynomials are stored in the GPU's constant memory
\begin{verbatim}
  __constant__ float legendre[3][3];
\end{verbatim}
and are passed to the \texttt{eval1dLegendre0()} and \texttt{eval1dLegendre1()} function calls to evaluate the Legendre polynomial and its first derivative and the function declaration is
\begin{verbatim}
  __device__ float eval1dLegendre0(float legendre[][3], int n, float x);
\end{verbatim}
After porting the code, while the declaration of the constant memory was changed to
\begin{verbatim}
  dpct::constant_memory<float, 2> legendre(3, 3);
\end{verbatim}
and it was included in the kernel call as
\begin{verbatim}
  dpct::accessor<float, dpct::constant, 2> legendre
\end{verbatim}
the type was not changed in the \texttt{eval1dLegendre} function.
After changing the parameter type for the first argument in the function definition from \texttt{float legendre[][3]} to \texttt{dpct::accessor<float, dpct::constant, 2> legendre} the ported code compiled succesfully.

\subsubsection{Running the code}
To start with, the simplified native CUDA code was tested on a workstation with an Nvidia 2060 (Turing) and in an HPC GPU cluster with A100's (Ampere). In the first case we used CUDA SDK 11.6 and on the GPU cluster we compiled with CUDA SDK 11.4. We also compiled it with the optimization levels \texttt{-O0} up to \texttt{-O3} and with or without the \texttt{-use\_fast\_math} flag and in all cases the unit test passed with no errors.

Now, for the ported SYCL/DPC++ code multiple different platforms were used, both CPU based and GPU based. The GPU was once again the Nvidia Ampere A100 and the CPU based systems were one with dual Intel Icelake 8368Q and one with dual AMD EPYC 7763. To target the Nvidia GPU we compiled with the LLVM CUDA backend compiler with \texttt{clang++ -fsycl -fsycl-targets=nvptx64-nvidia-cuda} and to target the CPU we compiled with both the DPC++ compiler (\texttt{dpcpp}) and with Intel's LLVM with \texttt{clang++ -fsycl} (which by default selects the generic spir64 target). The results of the unit test can be found on Table \ref{tab:correctness}.

\begin{center}
	\begin{table}
		\begin{tabular}{||c c c c c||}
			\hline
			    & \multicolumn{2}{c}{dpcpp} & \multicolumn{2}{c||}{clang++}                               \\ [0.5ex]
			\hline
			    & Icelake 8368Q             & EPYC 7763                     & Icelake 8368Q & Ampere A100 \\ [0.5ex]
			\hline\hline
			-O0 & 0.545073\% wrong          & 0.545073\% wrong              & -             & correct     \\ [0.3ex]
			\hline
			-O1 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
			-O2 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
			-O3 & 1.20335\% wrong           & 1.20545\% wrong               & correct       & correct     \\ [0.3ex]
			\hline
		\end{tabular}
		\caption{\label{tab:correctness}}
	\end{table}
\end{center}

Further investigation suggests that this behaviour is due to different default semantics for floating point calculations between the two different compilers. According to Table \ref{tab:semantics} the default model in \texttt{dpcpp} is \texttt{fast} while in \texttt{clang++} is \texttt{precise}.

\begin{center}
	\begin{table}
		\centering
		\begin{tabular}{||ll|l||}
			\hline
			\multirow{4}{*}{dpcpp -O1}          & ---                & 1.20335\% wrong  \\
			                                    & -fp-model=fast     & 1.20335\% wrong  \\
			                                    & -fp-model=strict   & 0.545073\% wrong \\
			                                    & -fp-model=precise  & correct          \\ \hline \hline
			\multirow{4}{*}{clang++ -fsycl -O1} & ---                & correct          \\
			                                    & -ffp-model=fast    & 1.20335\% wrong  \\
			                                    & -ffp-model=strict  & 0.545073\% wrong \\
			                                    & -ffp-model=precise & correct          \\ \hline
		\end{tabular}
		\caption{\label{tab:semantics}}
	\end{table}
\end{center}

\subsection{AREPO}

Arepo is a massively parallel astrophysics code for the simulation of gravitational N-body systems and magnetohydrodynamics, both on Newtonian as well as cosmological backgrounds. There are a number of versions in the community, originating from a common closed-source code base. A version with reduced functionality was made publicly available under GPLv3\cite{weinberger_arepo_2020, springel_arepo_nodate}. Different research groups use code bases derived from the closed source version, including the Sijaki group at the University of Cambridge. This code \cite{sijaki_arepo_nodate}, which was part of the DiRAC3 procurement, acceptance testing and technical commissioning, forms the basis of porting efforts to be undertaken within the course of this project.

In Arepo, the computational domain is discretized using a fully adaptive, dynamic unstructured Voronoi mesh, which is moving with the fluid in a quasi-Lagrangian way and that is paired with a finite volume approach for the hydrodynamics.
Arepo is written in C, parallelized with MPI and some of the code bases, including the one used in this project, have additionally OpenMP shared memory parallelization. To the best of our knowledge, there is currently no version of this code that can target accelerators with any of the computational kernels in the main time loop.
As a C application that includes already OpenMP parallelization, we intend to follow Intel's recommendation to use OpenMP 5 offloading to target GPUs.

%In order to run simulations efficiently on the next generation of supercomputers there are a few key aspects that need to be taken into account. Some of those steps are already well known in the HPC community for years, i.e. well balanced computational load or the avoiding collective communication as much as possible. Apart from that it will be increasingly important to make use of hybrid architectures, i.e. accelerators.
In contrast to codes described above, where suitable kernels for GPU execution have already been identified in prior porting efforts, this selection process has to be done as an additional first step for this purely CPU-based code. Therefore, extensive profiling of the code base has been undertaken using one of the test cases that are also used to verify code correctness to analyze the potential of porting parts of Arepo to GPUs. Hence, a first analysis was carried out using Intel VTune profiler and Intel Trace Analyzer, producing two key insights:

The first insight was that a significant share of the overall run time is spent waiting in a synchronisation step that uses global MPI communication. The purpose of this step is to detect the need for program interruption to trigger saving the current state in a snapshot file and termination of the program. Since there is no data dependence on this communication step, it is possible to reverse the logic of this routine, replace the respective MPI calls with their non-blocking counterparts and trigger the snapshot writing after the next time step. This enables perfect overlap of computation and communication at the expense of a one time step lag in the snapshot writing. Even on this small single-node test case this already reduces the run time of the main time loop by 2\%, promising even larger savings in production runs that use significantly more nodes.

\begin{figure}[htp]
	\centering
	\subfloat{ \includegraphics[clip,width=\textwidth]{arepo_blocking.png}}
	\subfloat{ \includegraphics[clip,width=\textwidth]{arepo_nonblocking.png}}
	\caption{Significant reduction of MPI Busy Wait Time (yellow) in the lower VTune Profiler graphic compared to the top version where blocking communication was used. Shown for a random selection of ranks}
	\label{fig:arepo_mpicom}
\end{figure}

Second profiling outcome and original motivation for this analysis was to understand the distribution of run time across the different parts of a time step and subsequently to identify suitable candidates for GPU porting. The two most time-consuming parts proved to be the evaluation of gravitational forces, which is carried out in two half steps at the beginning and end of a time step, and the creation of the Voronoi mesh in-between.

The next step in the analysis was to use Intel's Offload Advisor to see if these contain any kernels that would readily benefit from GPU offloading. However, with the current structure of the underlying algorithm the Offload Advisor judged every routine to suffer from too much overhead when ported to GPU. Only relatively short loops within sub steps of the mesh creation routines were classified as offload candidates that could potentially see any speed-up.

Overall, this suggests that for a successful GPU port of Arepo it may not be sufficient to port few individual kernels to achieve performance gains but requires a better understanding of the numerical method and possibly adaptation of the algorithm to better map to the target hardware. For that, we are in active discussions with the developers at the University of Cambridge and take into account prior experience and publications in the scientific domain.

%\begin{itemize}
%    \item reference to paper to port
%    \item https://arxiv.org/abs/1610.07279
%    \item https://arxiv.org/abs/0907.3390
%    \item https://arxiv.org/abs/1909.07439
%\end{itemize}
