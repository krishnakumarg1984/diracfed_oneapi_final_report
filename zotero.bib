@software{springel_arepo_nodate,
	title = {{AREPO}},
	url = {https://gitlab.mpcdf.mpg.de/vrs/arepo},
	author = {Springel, Volker},
}

@software{hemelb_authors_hemelb_nodate,
	title = {{HemeLB}: Haemodynamic simulation with lattice Boltzmann},
	url = {https://github.com/hemelb-codes/hemelb},
	abstract = {A high performance parallel lattice-Boltzmann code for large scale fluid flow in complex geometries},
	author = {{HemeLB} Authors},
}

@article{Nelson_2015,
	doi = {10.1016/j.ascom.2015.09.003},
	url = {https://doi.org/10.1016%2Fj.ascom.2015.09.003},
	year = 2015,
	month = {nov},
	publisher = {Elsevier {BV}},
	volume = {13},
	pages = {12--37},
	author = {D. Nelson and A. Pillepich and S. Genel and M. Vogelsberger and V. Springel and P. Torrey and V. Rodriguez-Gomez and D. Sijacki and G.F. Snyder and B. Griffen and F. Marinacci and L. Blecha and L. Sales and D. Xu and L. Hernquist},
	title = {The illustris simulation: Public data release},
	journal = {Astronomy and Computing}
}
@article{dong_gpu-accelerated_2021,
	title = {{GPU}-Accelerated Discontinuous Galerkin Methods on Polytopic Meshes},
	volume = {43},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/20M1350984},
	doi = {10.1137/20M1350984},
	abstract = {Discontinuous Galerkin ({dG}) methods on meshes consisting of polygonal/polyhedral (henceforth, collectively termed as polytopic) elements have received considerable attention in recent years. Due to the physical frame basis functions used typically and the quadrature challenges involved, the matrix-assembly step for these methods is often computationally cumbersome. To address this important practical issue, this work proposes two parallel assembly implementation algorithms on Compute Unified Device Architecture--enabled graphics cards for the interior penalty {dG} method on polytopic meshes for various classes of linear {PDE} problems. We are concerned with both single graphics processing unit ({GPU}) parallelization, as well as with implementation on distributed {GPU} nodes. The results included showcase almost linear scalability of the quadrature step with respect to the number of {GPU} cores used since no communication is needed for the assembly step. In turn, this can justify the claim that polytopic {dG} methods can be implemented extremely efficiently, as any assembly computing time overhead compared to finite elements on “standard” simplicial or box-type meshes can be effectively circumvented by the proposed algorithms.},
	pages = {C312--C334},
	number = {4},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	shortjournal = {{SIAM} J. Sci. Comput.},
	author = {Dong, Zhaonan and Georgoulis, Emmanuil H. and Kappas, Thomas},
	urldate = {2022-03-31},
	date = {2021-01-01},
	keywords = {68Q25, 68R10, 68U05, {GPU}, discontinuous Galerkin, high order methods, polytopic meshes},
}

@software{noauthor_openmm_nodate,
	title = {{OpenMM}},
	url = {https://github.com/openmm/openmm},
}

@software{fastsum_collaboration_openqcd-fastsum_nodate,
	title = {{OpenQCD}-{FASTSUM}},
	url = {https://gitlab.com/fastsum/openqcd-fastsum},
	abstract = {An extension to the {openQCD} lattice code by the {FASTSUM} collaboration},
	author = {{FASTSUM} collaboration},
}

@article{zacharoudiou_development_2022,
	title = {Development and performance of a {HemeLB} {GPU} code for human-scale blood flow simulation},
	url = {http://arxiv.org/abs/2202.11770},
	abstract = {In recent years, it has become increasingly common for high performance computers ({HPC}) to possess some level of heterogeneous architecture - typically in the form of {GPU} accelerators. In some machines these are isolated within a dedicated partition, whilst in others they are integral to all compute nodes - often with multiple {GPUs} per node - and provide the majority of a machine's compute performance. In light of this trend, it is becoming essential that codes deployed on {HPC} are updated to execute on accelerator hardware. In this paper we introduce a {GPU} implementation of the 3D blood flow simulation code {HemeLB} that has been developed using {CUDA} C++. We demonstrate how taking advantage of {NVIDIA} {GPU} hardware can achieve significant performance improvements compared to the equivalent {CPU} only code on which it has been built whilst retaining the excellent strong scaling characteristics that have been repeatedly demonstrated by the {CPU} version. With {HPC} positioned on the brink of the exascale era, we use {HemeLB} as a motivation to provide a discussion on some of the challenges that many users will face when deploying their own applications on upcoming exascale machines.},
	journaltitle = {{arXiv}:2202.11770 [physics]},
	author = {Zacharoudiou, I. and {McCullough}, J. W. S. and Coveney, P. V.},
	urldate = {2022-03-30},
	date = {2022-01-13},
	eprinttype = {arxiv},
	eprint = {2202.11770},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Physics - Fluid Dynamics},
}

@inproceedings{christgau_porting_2020,
	location = {New Orleans, {LA}, {USA}},
	title = {Porting a Legacy {CUDA} Stencil Code to {oneAPI}},
	isbn = {9781728174457},
	url = {https://ieeexplore.ieee.org/document/9150323/},
	doi = {10.1109/IPDPSW50202.2020.00070},
	eventtitle = {2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops ({IPDPSW})},
	pages = {359--367},
	booktitle = {2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops ({IPDPSW})},
	publisher = {{IEEE}},
	author = {Christgau, Steffen and Steinke, Thomas},
	urldate = {2022-03-29},
	date = {2020-05},
}

@inproceedings{alekseenko_experiences_2021,
	location = {New York, {NY}, {USA}},
	title = {Experiences With Adding {SYCL} Support to {GROMACS}},
	isbn = {9781450390330},
	url = {https://doi.org/10.1145/3456669.3456690},
	doi = {10.1145/3456669.3456690},
	series = {{IWOCL}'21},
	abstract = {{GROMACS} is an open-source, high-performance molecular dynamics ({MD}) package primarily used for biomolecular simulations, accounting for 5\% of {HPC} utilization worldwide. Due to the extreme computing needs of {MD}, significant efforts are invested in improving the performance and scalability of simulations. Target hardware ranges from supercomputers to laptops of individual researchers and volunteers of distributed computing projects such as [email protected] The code has been designed both for portability and performance by explicitly adapting algorithms to {SIMD} and data-parallel processors. A {SIMD} intrinsic abstraction layer provides high {CPU} performance. Explicit {GPU} acceleration has long used {CUDA} to target {NVIDIA} devices and {OpenCL} for {AMD}/Intel devices. In this talk, we discuss the experiences and challenges of adding support for the {SYCL} platform into the established {GROMACS} codebase and share experiences and considerations in porting and optimization. While {OpenCL} offers the benefits of using the same code to target different hardware, it suffers from several drawbacks that add significant development friction. Its separate-source model leads to code duplication and makes changes complicated. The need to use C99 for kernels, while the rest of the codebase uses C++17, exacerbates these issues. Another problem is that {OpenCL}, while supported by most {GPU} vendors, is never the main framework and thus is not getting the primary support or tuning efforts. {SYCL} alleviates many of these issues, employing a single-source model based on the modern C++ standard. In addition to being the primary platform for Intel {GPUs}, the possibility to target {AMD} and {NVIDIA} {GPUs} through other implementations (e.g., {hipSYCL}) might make it possible to reduce the number of separate {GPU} ports that have to be maintained. Some design differences from {OpenCL}, such as flow directed acyclic graphs ({DAGs}) instead of in-order queues, made it necessary to reconsider the {GROMACS}’s task scheduling approach and architectural choices in the {GPU} backend. Additionally, supporting multiple {GPU} platforms presents a challenge of balancing performance (low-level and hardware-specific code) and maintainability (more generalization and code-reuse). We will discuss the limitations of the existing codebase and interoperability layers with regards to adding the new platform; the compute performance and latency comparisons; code quality considerations; and the issues we encountered with {SYCL} implementations tested. Finally, we will discuss our goals for the next release cycle for the {SYCL} backend and the overall architecture of {GPU} acceleration code in {GROMACS}.},
	pages = {1},
	booktitle = {International Workshop on {OpenCL}},
	publisher = {Association for Computing Machinery},
	author = {Alekseenko, Andrey and Páll, Szilárd and Lindahl, Erik},
	urldate = {2022-03-29},
	date = {2021-04-27},
	keywords = {{GROMACS}, Heterogeneous acceleration, {SYCL}},
}

@online{intel_corp_oneapi_nodate,
	title = {{oneAPI}: A New Era of Heterogeneous Computing},
	url = {https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html},
	author = {Intel Corp.},
}

@online{kronos_group_sycl_nodate,
	title = {{SYCL} - C++ Single-source Heterogeneous Programming for Acceleration Offload},
	url = {https://www.khronos.org/sycl/},
	author = {Kronos Group},
}

@online{dirac_distributed_nodate,
	title = {Distributed Research utilising Advanced Computing},
	url = {https://dirac.ac.uk},
	shorttitle = {{DiRAC}},
	author = {{DiRAC}},
}

@software{sijaki_arepo_nodate,
	title = {{AREPO} Source Code for {DiRAC}3 Benchmarking},
	url = {https://github.com/DiRAC-HPC/di-benchmarking-dell/tree/main/arepo},
	publisher = {University of Cambridge},
	author = {Sijaki, Debora},
}

@article{weinberger_arepo_2020,
	title = {The {AREPO} Public Code Release},
	volume = {248},
	issn = {1538-4365},
	url = {https://iopscience.iop.org/article/10.3847/1538-4365/ab908c},
	doi = {10.3847/1538-4365/ab908c},
	pages = {32},
	number = {2},
	journaltitle = {The Astrophysical Journal Supplement Series},
	shortjournal = {{ApJS}},
	author = {Weinberger, Rainer and Springel, Volker and Pakmor, Rüdiger},
	urldate = {2022-03-23},
	date = {2020-06-10},
}

@article{costanzo_early_2021,
	title = {Early Experiences Migrating {CUDA} codes to {oneAPI}},
	url = {http://arxiv.org/abs/2105.13489},
	abstract = {The heterogeneous computing paradigm represents a real programming challenge due to the proliferation of devices with different hardware characteristics. Recently Intel introduced {oneAPI}, a new programming environment that allows code developed in {DPC}++ to be run on different devices such as {CPUs}, {GPUs}, {FPGAs}, among others. This paper presents our first experiences in porting two {CUDA} applications to {DPC}++ using the {oneAPI} dpct tool. From the experimental work, it was possible to verify that dpct does not achieve 100\% of the migration task; however, it performs most of the work, reporting the programmer of possible pending adaptations. Additionally, it was possible to verify the functional portability of the {DPC}++ code obtained, having successfully executed it on different {CPU} and {GPU} architectures.},
	journaltitle = {{arXiv}:2105.13489 [cs]},
	author = {Costanzo, Manuel and Rucci, Enzo and Sanchez, Carlos García and Naiouf, Marcelo},
	urldate = {2022-03-23},
	date = {2021-05-27},
	eprinttype = {arxiv},
	eprint = {2105.13489},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{costanzo_migrating_2022,
	title = {Migrating {CUDA} to {oneAPI}: A Smith-Waterman Case Study},
	url = {http://arxiv.org/abs/2203.11100},
	shorttitle = {Migrating {CUDA} to {oneAPI}},
	abstract = {To face the programming challenges related to heterogeneous computing, Intel recently introduced {oneAPI}, a new programming environment that allows code developed in Data Parallel C++ ({DPC}++) language to be run on different devices such as {CPUs}, {GPUs}, {FPGAs}, among others. To tackle {CUDA}-based legacy codes, {oneAPI} provides a compatibility tool (dpct) that facilitates the migration to {DPC}++. Due to the large amount of existing {CUDA}-based software in the bioinformatics context, this paper presents our experiences porting {SW}\#db, a well-known sequence alignment tool, to {DPC}++ using dpct. From the experimental work, it was possible to prove the usefulness of dpct for {SW}\#db code migration and the cross-{GPU} vendor, cross-architecture portability of the migrated {DPC}++ code. In addition, the performance results showed that the migrated {DPC}++ code reports similar efficiency rates to its {CUDA}-native counterpart or even better in some tests (approximately +5\%).},
	journaltitle = {{arXiv}:2203.11100 [cs]},
	author = {Costanzo, Manuel and Rucci, Enzo and Sanchez, Carlos Garcia and Naiouf, Marcelo and Prieto-Matias, Manuel},
	urldate = {2022-03-23},
	date = {2022-03-21},
	eprinttype = {arxiv},
	eprint = {2203.11100},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
}
